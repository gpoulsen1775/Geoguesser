{"cells":[{"cell_type":"markdown","metadata":{"id":"heFkUGiawcbO"},"source":["<h1> Define train_test function and a load function </h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1fVU8cL6NntO"},"outputs":[],"source":["import os\n","import cv2\n","import keras\n","import pickle\n","import numpy as np\n","import pandas as pd\n","\n","def train_test_split(rd,train_ratio):\n","\n","    fls = np.asarray(os.listdir(rd)) #Get Files\n","    fls = np.delete(fls,np.where(fls == '.DS_Store')) #Remove Store File\n","    np.random.shuffle(fls) #Get IID Arrays\n","\n","    train_length = int(np.ceil(train_ratio * (len(fls))))\n","    test_length = len(fls) - train_length\n","\n","    train = fls[0:train_length]\n","    test = fls[-test_length:]\n","\n","    return train, test\n","\n","def load(rd,file_names):\n","\n","    label_map = {\n","        'Georgia':0,\n","        'Idaho':1,\n","        'Maine':2,\n","        'Utah':3 }\n","\n","    X = np.zeros(shape = (len(file_names),150,150,3),dtype=np.float16)\n","    Y = np.zeros(shape=len(file_names), dtype=int)\n","\n","    for idx,sub_dir in enumerate(file_names):\n","\n","        # if('.DS' in sub_dir):\n","        #     continue\n","\n","        dr = os.path.join(rd,sub_dir)\n","        im = cv2.imread(os.path.join(dr,\"gsv_0.jpg\"))\n","        im = im / 255\n","\n","        X[idx,:,:,:] = im\n","        Y[idx] = label_map[sub_dir.split(\" \")[0]]\n","\n","        # meta = os.path.join(dr,'metadata.json')\n","        # if(json.load(open(meta))[0]['status'] == 'OK'):\n","\n","    return X, Y\n","\n","def load_set(fp):\n","\n","  with open(fp,'rb') as fl:\n","    return pickle.load(fl)\n","\n","def save_set(fp,l):\n","\n","  with open(fp,'wb') as fl:\n","    pickle.dump(l,fl)"]},{"cell_type":"markdown","metadata":{"id":"rpGqbnZz3kW4"},"source":["<h1> Setup Data Storage and Fetching </h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AtawPSHNwTUF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687141629922,"user_tz":360,"elapsed":165,"user":{"displayName":"GP","userId":"02123881851617326847"}},"outputId":"d51881da-98e7-4fc6-df4e-0aa6ff251444"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'Geoguesser' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/gpoulsen1775/Geoguesser.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AA3fqfoP3j7e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687141630652,"user_tz":360,"elapsed":733,"user":{"displayName":"GP","userId":"02123881851617326847"}},"outputId":"5397fe1b-c9c5-4f68-ef2d-6f619f653385"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"1IillrCSwjud"},"source":["<h1> Load in train-test data </h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VjspFtnNntR"},"outputs":[],"source":["rd = '/content/drive/MyDrive/Colab Notebooks/Geoguesser/'\n","fns = ['X_train','Y_train','X_test','Y_test']\n","\n","try: #Try to Load\n","\n","  rs = []\n","  for fn in fns:\n","\n","    rs.append(load_set(rd + fn))\n","\n","  X_train,Y_train,X_test,Y_test = rs\n","\n","except: #Create & Save\n","\n","  trn, tst = train_test_split('/content/Geoguesser/extracted_data',.8) #Break Directories into a train and test set\n","  X_train,Y_train = load(\"/content/Geoguesser/extracted_data\",trn) #Load the data set\n","  X_test,Y_test = load(\"/content/Geoguesser/extracted_data\",tst) #Load the data set\n","\n","  data = [X_train,Y_train,X_test,Y_test]\n","\n","  for i,fn in enumerate(fns):\n","\n","    save_set(rd+fn,data[i])"]},{"cell_type":"markdown","metadata":{"id":"xAaeiafqwoJ-"},"source":["<h1> Define VGG16 Model </h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wSi-RMYHPwoL"},"outputs":[],"source":["# #VGG16 MAX 60%\n","# from keras.layers.serialization import activation\n","# from keras.models import Sequential\n","# from keras import regularizers\n","# from keras import initializers\n","# import keras.layers as kl\n","\n","# model =  Sequential()\n","\n","# model.add(kl.Conv2D(input_shape=(150,150,3), filters=8, kernel_size=(5,5), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Input\n","# model.add(kl.Conv2D(filters=8, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Convolution 1\n","# model.add(kl.MaxPool2D(pool_size=(2,2),strides=(2,2))) #Pool 2\n","# model.add(kl.BatchNormalization())\n","\n","# model.add(kl.Conv2D(filters=16, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Convolution 3\n","# model.add(kl.Conv2D(filters=16, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Convolution 4\n","# model.add(kl.MaxPool2D(pool_size=(2,2),strides=(2,2))) #Pool 5\n","# model.add(kl.BatchNormalization())\n","\n","# model.add(kl.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Convolution 6\n","# model.add(kl.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Convolution 7\n","# model.add(kl.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Convolution 8\n","# model.add(kl.MaxPool2D(pool_size=(2,2),strides=(2,2))) #Pool 9\n","# model.add(kl.BatchNormalization())\n","\n","# model.add(kl.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Convolution 10\n","# model.add(kl.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Convolution 11\n","# model.add(kl.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Convolution 12\n","# model.add(kl.MaxPool2D(pool_size=(2,2),strides=(2,2))) #Pool 13\n","# model.add(kl.BatchNormalization())\n","\n","# model.add(kl.Flatten()) #Flatten output\n","# model.add(kl.Dropout(0.4)) #Add Dropout to Layer\n","# model.add(kl.Dense(units=514, activation='relu', kernel_initializer=initializers.HeNormal())) #Fully Connected 14 #DOUBLED -> Works slightly faster\n","# model.add(kl.Dropout(0.4)) #Add Dropout to Layer\n","# model.add(kl.Dense(units=514, activation='relu', kernel_initializer=initializers.HeNormal())) #Fully Connected 15 #DOUBLED -> Works slightly faster\n","# model.add(kl.Dense(units=4, activation='softmax')) #Softmax Output\n","\n","# from keras.optimizers import Adam\n","\n","# opt = Adam(learning_rate=0.0003)\n","# model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQtZD3c9v9TX"},"outputs":[],"source":["#Best Complex ~4 CNN Layer with all the defaults starting at 16"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VQlH0OmUZl-_"},"outputs":[],"source":["#68% By 13\n","from keras.layers.serialization import activation\n","from keras.models import Sequential\n","from keras import regularizers\n","from keras import initializers\n","import keras.layers as kl\n","\n","model =  Sequential()\n","\n","#Layer 1\n","model.add(kl.Conv2D(input_shape=(150,150,3), filters=16, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01)))\n","model.add(kl.MaxPool2D(pool_size=(4,4),strides=(4,4)))\n","model.add(kl.BatchNormalization())\n","\n","#Layer 2\n","model.add(kl.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01)))\n","model.add(kl.MaxPool2D(pool_size=(3,3),strides=(3,3)))\n","model.add(kl.BatchNormalization())\n","\n","#Layer 3\n","model.add(kl.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01)))\n","model.add(kl.MaxPool2D(pool_size=(2,2),strides=(2,2)))\n","model.add(kl.BatchNormalization())\n","\n","#Layer 4\n","model.add(kl.Flatten())\n","model.add(kl.Dropout(0.4))\n","model.add(kl.Dense(units=32, activation='relu', kernel_initializer=initializers.HeNormal()))\n","model.add(kl.Dense(units=4, activation='softmax')) #Softmax Output\n","\n","from keras.optimizers import Adam\n","\n","opt = Adam(learning_rate=0.0005)\n","model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","source":["#TEST\n","from keras.layers.serialization import activation\n","from keras.models import Sequential\n","from keras import regularizers\n","from keras import initializers\n","import keras.layers as kl\n","\n","model =  Sequential()\n","\n","#Layer 1\n","model.add(kl.Conv2D(input_shape=(150,150,3), filters=16, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01)))\n","model.add(kl.MaxPool2D(pool_size=(4,4),strides=(4,4)))\n","model.add(kl.BatchNormalization())\n","\n","#Layer 2\n","model.add(kl.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01)))\n","model.add(kl.MaxPool2D(pool_size=(3,3),strides=(3,3)))\n","model.add(kl.BatchNormalization())\n","\n","#Layer 3\n","model.add(kl.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01)))\n","model.add(kl.MaxPool2D(pool_size=(2,2),strides=(2,2)))\n","model.add(kl.BatchNormalization())\n","\n","#Layer 4\n","model.add(kl.Flatten())\n","model.add(kl.Dropout(0.4))\n","model.add(kl.Dense(units=32, activation='relu', kernel_initializer=initializers.HeNormal()))\n","model.add(kl.Dense(units=4, activation='softmax')) #Softmax Output\n","\n","from keras.optimizers import Adam\n","\n","opt = Adam(learning_rate=0.0007)\n","model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"],"metadata":{"id":"hmihf9HrQYQO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# https://arxiv.org/pdf/2003.12843.pdf"],"metadata":{"id":"KaTPmb9Zb89K"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zhFX3poCYwfT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687141649568,"user_tz":360,"elapsed":10,"user":{"displayName":"GP","userId":"02123881851617326847"}},"outputId":"3c9c97df-b0c4-4ece-9a6d-23124b59d2b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_9 (Conv2D)           (None, 150, 150, 16)      448       \n","                                                                 \n"," max_pooling2d_9 (MaxPooling  (None, 37, 37, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," batch_normalization_9 (Batc  (None, 37, 37, 16)       64        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_10 (Conv2D)          (None, 37, 37, 32)        4640      \n","                                                                 \n"," max_pooling2d_10 (MaxPoolin  (None, 12, 12, 32)       0         \n"," g2D)                                                            \n","                                                                 \n"," batch_normalization_10 (Bat  (None, 12, 12, 32)       128       \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 12, 12, 64)        18496     \n","                                                                 \n"," max_pooling2d_11 (MaxPoolin  (None, 6, 6, 64)         0         \n"," g2D)                                                            \n","                                                                 \n"," batch_normalization_11 (Bat  (None, 6, 6, 64)         256       \n"," chNormalization)                                                \n","                                                                 \n"," flatten_3 (Flatten)         (None, 2304)              0         \n","                                                                 \n"," dropout_3 (Dropout)         (None, 2304)              0         \n","                                                                 \n"," dense_6 (Dense)             (None, 32)                73760     \n","                                                                 \n"," dense_7 (Dense)             (None, 4)                 132       \n","                                                                 \n","=================================================================\n","Total params: 97,924\n","Trainable params: 97,700\n","Non-trainable params: 224\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"eOSj7YVCwsoE"},"source":["<h1> Train Model </h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9NDoFhINntT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1d84c99d-ba2c-4863-f896-a17f43753b34"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/400\n","110/110 [==============================] - 56s 508ms/step - loss: 1.9293 - accuracy: 0.4294 - val_loss: 2.0449 - val_accuracy: 0.2641\n","Epoch 2/400\n","110/110 [==============================] - 54s 495ms/step - loss: 1.6127 - accuracy: 0.5424 - val_loss: 2.8719 - val_accuracy: 0.2538\n","Epoch 3/400\n","110/110 [==============================] - 52s 471ms/step - loss: 1.4320 - accuracy: 0.5991 - val_loss: 2.2458 - val_accuracy: 0.3564\n","Epoch 4/400\n","110/110 [==============================] - 54s 486ms/step - loss: 1.2952 - accuracy: 0.6334 - val_loss: 1.7057 - val_accuracy: 0.4744\n","Epoch 5/400\n","110/110 [==============================] - 51s 467ms/step - loss: 1.1790 - accuracy: 0.6742 - val_loss: 1.4460 - val_accuracy: 0.5487\n","Epoch 6/400\n","110/110 [==============================] - 54s 490ms/step - loss: 1.1014 - accuracy: 0.6956 - val_loss: 1.3926 - val_accuracy: 0.5667\n","Epoch 7/400\n","110/110 [==============================] - 52s 476ms/step - loss: 0.9924 - accuracy: 0.7290 - val_loss: 1.3225 - val_accuracy: 0.5769\n","Epoch 8/400\n"," 68/110 [=================>............] - ETA: 23s - loss: 0.9440 - accuracy: 0.7555"]}],"source":["from keras.models import load_model\n","\n","EPOCHS = 400\n","checkpoint_filepath = '/content/drive/MyDrive/Colab Notebooks/Geoguesser/best_model.hdf5'\n","history_file = '/content/drive/MyDrive/Colab Notebooks/Geoguesser/log.csv'\n","\n","#Make History Saver\n","history_callback = keras.callbacks.CSVLogger(history_file, separator=\",\", append=True)\n","\n","#Make Checkpoint\n","checkpoint_callback = keras.callbacks.ModelCheckpoint(\n","\n","    filepath=checkpoint_filepath,\n","    monitor='val_loss',\n","    mode='min',\n","    save_best_only=True,\n","    save_weights_only=False)\n","\n","#Make Early Stop\n","early_stop_callback = keras.callbacks.EarlyStopping(\n","\n","    monitor='val_loss',\n","    patience=10,\n","    restore_best_weights=True)\n","\n","#Try to load the model on the chance that the model has saved progress\n","try:\n","  model = load_model(checkpoint_filepath)\n","  log = pd.read_csv(history_file)\n","\n","  with pd.option_context('display.max_rows', None,\n","                       'display.max_columns', None,\n","                       'display.precision', 3,):\n","    print(log)\n","\n","  EPOCHS = EPOCHS - len(log)\n","\n","#If the saved model doesn't exist, then continue on...\n","except:\n","  pass\n","\n","# Model weights are saved at the end of every epoch, if it's the best seen\n","# so far.\n","history = model.fit(x=X_train, y=Y_train, epochs=EPOCHS, callbacks=[checkpoint_callback,early_stop_callback,history_callback], validation_split=.1)\n","\n","#Try Fit Generator if Data Augmentation is needed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UH5zuq-FgnDQ"},"outputs":[],"source":["model = load_model(checkpoint_filepath)\n"]},{"cell_type":"code","source":["rd = '/content/drive/MyDrive/Colab Notebooks/Geoguesser/'\n","fns = ['X_train','Y_train','X_test','Y_test']\n","\n","rs = []\n","for fn in fns:\n","\n","  rs.append(load_set(rd + fn))\n","\n","X_train,Y_train,X_test,Y_test = rs"],"metadata":{"id":"oNQrFV_hYpJl"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"},"kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}