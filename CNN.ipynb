{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heFkUGiawcbO"
      },
      "source": [
        "<h1> Define train_test function and a load function </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1fVU8cL6NntO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import keras\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def train_test_split(rd,train_ratio):\n",
        "\n",
        "    fls = np.asarray(os.listdir(rd)) #Get Files\n",
        "    fls = np.delete(fls,np.where(fls == '.DS_Store')) #Remove Store File\n",
        "    np.random.shuffle(fls) #Get IID Arrays\n",
        "\n",
        "    train_length = int(np.ceil(train_ratio * (len(fls))))\n",
        "    test_length = len(fls) - train_length\n",
        "\n",
        "    train = fls[0:train_length]\n",
        "    test = fls[-test_length:]\n",
        "\n",
        "    return train, test\n",
        "\n",
        "def load(rd,file_names):\n",
        "\n",
        "    label_map = {\n",
        "        'Georgia':0,\n",
        "        'Idaho':1,\n",
        "        'Maine':2,\n",
        "        'Utah':3 }\n",
        "\n",
        "    X = np.zeros(shape = (len(file_names),150,150,3),dtype=np.float16)\n",
        "    Y = np.zeros(shape=len(file_names), dtype=int)\n",
        "\n",
        "    for idx,sub_dir in enumerate(file_names):\n",
        "\n",
        "        # if('.DS' in sub_dir):\n",
        "        #     continue\n",
        "\n",
        "        dr = os.path.join(rd,sub_dir)\n",
        "        im = cv2.imread(os.path.join(dr,\"gsv_0.jpg\"))\n",
        "        im = im / 255\n",
        "\n",
        "        X[idx,:,:,:] = im\n",
        "        Y[idx] = label_map[sub_dir.split(\" \")[0]]\n",
        "\n",
        "        # meta = os.path.join(dr,'metadata.json')\n",
        "        # if(json.load(open(meta))[0]['status'] == 'OK'):\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "def load_set(fp):\n",
        "\n",
        "  with open(fp,'rb') as fl:\n",
        "    return pickle.load(fl)\n",
        "\n",
        "def save_set(fp,l):\n",
        "\n",
        "  with open(fp,'wb') as fl:\n",
        "    pickle.dump(l,fl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpGqbnZz3kW4"
      },
      "source": [
        "<h1> Setup Data Storage and Fetching </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtawPSHNwTUF",
        "outputId": "da4b7d29-7ebc-43e9-c0c4-1e7f5b13ba97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Geoguesser'...\n",
            "remote: Enumerating objects: 12912, done.\u001b[K\n",
            "remote: Counting objects: 100% (12912/12912), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12907/12907), done.\u001b[K\n",
            "remote: Total 12912 (delta 3), reused 12912 (delta 3), pack-reused 0\n",
            "Receiving objects: 100% (12912/12912), 26.30 MiB | 12.57 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "Updating files: 100% (9742/9742), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/gpoulsen1775/Geoguesser.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA3fqfoP3j7e",
        "outputId": "f7e9cd6f-e5dc-46a6-ed30-acaed4319e66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IillrCSwjud"
      },
      "source": [
        "<h1> Load in train-test data </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4VjspFtnNntR"
      },
      "outputs": [],
      "source": [
        "rd = '/content/drive/MyDrive/Colab Notebooks/Geoguesser/'\n",
        "fns = ['X_train','Y_train','X_test','Y_test']\n",
        "\n",
        "try: #Try to Load\n",
        "\n",
        "  rs = []\n",
        "  for fn in fns:\n",
        "\n",
        "    rs.append(load_set(rd + fn))\n",
        "\n",
        "  X_train,Y_train,X_test,Y_test = rs\n",
        "\n",
        "except: #Create & Save\n",
        "\n",
        "  trn, tst = train_test_split('/content/Geoguesser/extracted_data',.8) #Break Directories into a train and test set\n",
        "  X_train,Y_train = load(\"/content/Geoguesser/extracted_data\",trn) #Load the data set\n",
        "  X_test,Y_test = load(\"/content/Geoguesser/extracted_data\",tst) #Load the data set\n",
        "\n",
        "  data = [X_train,Y_train,X_test,Y_test]\n",
        "\n",
        "  for i,fn in enumerate(fns):\n",
        "\n",
        "    save_set(rd+fn,data[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAaeiafqwoJ-"
      },
      "source": [
        "<h1> Define VGG16 Model </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSi-RMYHPwoL"
      },
      "outputs": [],
      "source": [
        "# #VGG16 MAX 60%\n",
        "# from keras.layers.serialization import activation\n",
        "# from keras.models import Sequential\n",
        "# from keras import regularizers\n",
        "# from keras import initializers\n",
        "# import keras.layers as kl\n",
        "\n",
        "# model =  Sequential()\n",
        "\n",
        "# model.add(kl.Conv2D(input_shape=(150,150,3), filters=8, kernel_size=(5,5), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Input\n",
        "# model.add(kl.Conv2D(filters=8, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Convolution 1\n",
        "# model.add(kl.MaxPool2D(pool_size=(2,2),strides=(2,2))) #Pool 2\n",
        "# model.add(kl.BatchNormalization())\n",
        "\n",
        "# model.add(kl.Conv2D(filters=16, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Convolution 3\n",
        "# model.add(kl.Conv2D(filters=16, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Convolution 4\n",
        "# model.add(kl.MaxPool2D(pool_size=(2,2),strides=(2,2))) #Pool 5\n",
        "# model.add(kl.BatchNormalization())\n",
        "\n",
        "# model.add(kl.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Convolution 6\n",
        "# model.add(kl.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Convolution 7\n",
        "# model.add(kl.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Convolution 8\n",
        "# model.add(kl.MaxPool2D(pool_size=(2,2),strides=(2,2))) #Pool 9\n",
        "# model.add(kl.BatchNormalization())\n",
        "\n",
        "# model.add(kl.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Convolution 10\n",
        "# model.add(kl.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Convolution 11\n",
        "# model.add(kl.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01))) #Convolution 12\n",
        "# model.add(kl.MaxPool2D(pool_size=(2,2),strides=(2,2))) #Pool 13\n",
        "# model.add(kl.BatchNormalization())\n",
        "\n",
        "# model.add(kl.Flatten()) #Flatten output\n",
        "# model.add(kl.Dropout(0.4)) #Add Dropout to Layer\n",
        "# model.add(kl.Dense(units=514, activation='relu', kernel_initializer=initializers.HeNormal())) #Fully Connected 14 #DOUBLED -> Works slightly faster\n",
        "# model.add(kl.Dropout(0.4)) #Add Dropout to Layer\n",
        "# model.add(kl.Dense(units=514, activation='relu', kernel_initializer=initializers.HeNormal())) #Fully Connected 15 #DOUBLED -> Works slightly faster\n",
        "# model.add(kl.Dense(units=4, activation='softmax')) #Softmax Output\n",
        "\n",
        "# from keras.optimizers import Adam\n",
        "\n",
        "# opt = Adam(learning_rate=0.0003)\n",
        "# model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQlH0OmUZl-_"
      },
      "outputs": [],
      "source": [
        "# #69% By 13\n",
        "# from keras.layers.serialization import activation\n",
        "# from keras.models import Sequential\n",
        "# from keras import regularizers\n",
        "# from keras import initializers\n",
        "# import keras.layers as kl\n",
        "\n",
        "# model =  Sequential()\n",
        "\n",
        "# #Layer 1\n",
        "# model.add(kl.Conv2D(input_shape=(150,150,3), filters=16, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01)))\n",
        "# model.add(kl.MaxPool2D(pool_size=(4,4),strides=(4,4)))\n",
        "# model.add(kl.BatchNormalization())\n",
        "\n",
        "# #Layer 2\n",
        "# model.add(kl.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01)))\n",
        "# model.add(kl.MaxPool2D(pool_size=(3,3),strides=(3,3)))\n",
        "# model.add(kl.BatchNormalization())\n",
        "\n",
        "# #Layer 3\n",
        "# model.add(kl.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01)))\n",
        "# model.add(kl.MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "# model.add(kl.BatchNormalization())\n",
        "\n",
        "# #Layer 4\n",
        "# model.add(kl.Flatten())\n",
        "# model.add(kl.Dropout(0.4))\n",
        "# model.add(kl.Dense(units=32, activation='relu', kernel_initializer=initializers.HeNormal()))\n",
        "# model.add(kl.Dense(units=4, activation='softmax')) #Softmax Output\n",
        "\n",
        "# from keras.optimizers import Adam\n",
        "\n",
        "# opt = Adam(learning_rate=0.0005)\n",
        "# model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hmihf9HrQYQO"
      },
      "outputs": [],
      "source": [
        "#72% By 30. Slower, but uses less training accuracy to get as high of val_accuracy\n",
        "#74 By 75\n",
        "from keras.layers.serialization import activation\n",
        "from keras.models import Sequential\n",
        "from keras import regularizers\n",
        "from keras import initializers\n",
        "import keras.layers as kl\n",
        "\n",
        "model =  Sequential()\n",
        "\n",
        "#Layer 1\n",
        "model.add(kl.Conv2D(input_shape=(150,150,3), filters=32, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01)))\n",
        "model.add(kl.MaxPool2D(pool_size=(4,4),strides=(4,4)))\n",
        "model.add(kl.BatchNormalization())\n",
        "\n",
        "#Layer 2\n",
        "model.add(kl.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01)))\n",
        "model.add(kl.MaxPool2D(pool_size=(3,3),strides=(3,3)))\n",
        "model.add(kl.BatchNormalization())\n",
        "\n",
        "#Layer 3\n",
        "model.add(kl.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01)))\n",
        "model.add(kl.MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(kl.BatchNormalization())\n",
        "\n",
        "#Layer 4\n",
        "model.add(kl.Flatten())\n",
        "model.add(kl.Dropout(0.7))\n",
        "model.add(kl.Dense(units=32, activation='relu', kernel_initializer=initializers.HeNormal()))\n",
        "model.add(kl.Dense(units=4, activation='softmax')) #Softmax Output\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "opt = Adam(learning_rate=0.0005)\n",
        "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WvFOuPHvJYXP"
      },
      "outputs": [],
      "source": [
        "#72% By 30. Slower, but uses less training accuracy to get as high of val_accuracy\n",
        "#74 By 75\n",
        "from keras.layers.serialization import activation\n",
        "from keras.models import Sequential\n",
        "from keras import regularizers\n",
        "from keras import initializers\n",
        "import keras.layers as kl\n",
        "\n",
        "model =  Sequential()\n",
        "\n",
        "#Layer 1\n",
        "model.add(kl.Conv2D(input_shape=(150,150,3), filters=32, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01)))\n",
        "model.add(kl.MaxPool2D(pool_size=(4,4),strides=(4,4)))\n",
        "model.add(kl.BatchNormalization())\n",
        "\n",
        "#Layer 2\n",
        "model.add(kl.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01)))\n",
        "model.add(kl.MaxPool2D(pool_size=(3,3),strides=(3,3)))\n",
        "model.add(kl.BatchNormalization())\n",
        "\n",
        "#Layer 3\n",
        "model.add(kl.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(l=0.01)))\n",
        "model.add(kl.MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(kl.BatchNormalization())\n",
        "\n",
        "#Layer 4\n",
        "model.add(kl.Flatten())\n",
        "model.add(kl.Dropout(0.5))\n",
        "model.add(kl.Dense(units=32, activation='relu', kernel_initializer=initializers.HeNormal()))\n",
        "model.add(kl.Dense(units=4, activation='softmax')) #Softmax Output\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "opt = Adam(learning_rate=0.0005)\n",
        "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KaTPmb9Zb89K"
      },
      "outputs": [],
      "source": [
        "# https://arxiv.org/pdf/2003.12843.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhFX3poCYwfT",
        "outputId": "8fff567e-7fe0-4227-8891-a516210ab753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 150, 150, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 37, 37, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 37, 37, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 37, 37, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 12, 12, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 12, 12, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 12, 12, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 6, 6, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 6, 6, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 4608)              0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4608)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 32)                147488    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 4)                 132       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 241,764\n",
            "Trainable params: 241,316\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOSj7YVCwsoE"
      },
      "source": [
        "<h1> Train Model </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G9NDoFhINntT",
        "outputId": "41b8f869-eda0-481a-b9d3-42148e07f8de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    epoch  accuracy   loss  val_accuracy  val_loss\n",
            "0       0     0.441  1.922         0.354     1.820\n",
            "1       1     0.546  1.508         0.415     2.031\n",
            "2       2     0.573  1.414         0.403     1.799\n",
            "3       3     0.627  1.284         0.449     1.542\n",
            "4       4     0.639  1.207         0.479     1.562\n",
            "5       5     0.645  1.150         0.567     1.286\n",
            "6       6     0.658  1.100         0.559     1.369\n",
            "7       7     0.675  1.031         0.582     1.188\n",
            "8       8     0.696  0.994         0.592     1.276\n",
            "9       9     0.706  0.938         0.621     1.159\n",
            "10     10     0.715  0.891         0.585     1.312\n",
            "11     11     0.744  0.848         0.613     1.166\n",
            "12     12     0.752  0.807         0.595     1.396\n",
            "13     13     0.741  0.803         0.597     1.235\n",
            "14     14     0.764  0.769         0.597     1.165\n",
            "15     15     0.769  0.758         0.605     1.260\n",
            "16     16     0.775  0.726         0.651     1.147\n",
            "17     17     0.798  0.682         0.562     1.523\n",
            "18     18     0.797  0.677         0.628     1.191\n",
            "19     19     0.803  0.640         0.613     1.314\n",
            "20     20     0.823  0.610         0.603     1.247\n",
            "21     21     0.827  0.600         0.626     1.289\n",
            "22      0     0.452  2.540         0.244     2.975\n",
            "23      1     0.539  2.164         0.318     2.787\n",
            "24      2     0.595  1.910         0.282     3.727\n",
            "25      3     0.624  1.729         0.318     2.724\n",
            "26      4     0.660  1.582         0.500     1.870\n",
            "27      5     0.676  1.438         0.628     1.592\n",
            "28      6     0.702  1.324         0.569     1.751\n",
            "29      7     0.731  1.199         0.585     1.560\n",
            "30      8     0.755  1.115         0.644     1.367\n",
            "31      9     0.777  1.032         0.633     1.347\n",
            "32     10     0.789  0.957         0.659     1.423\n",
            "33     11     0.814  0.889         0.633     1.374\n",
            "34     12     0.829  0.846         0.672     1.327\n",
            "35     13     0.837  0.795         0.636     1.326\n",
            "36     14     0.840  0.768         0.659     1.299\n",
            "37     15     0.861  0.708         0.646     1.348\n",
            "38     16     0.865  0.702         0.659     1.349\n",
            "39     17     0.889  0.643         0.674     1.358\n",
            "40     18     0.892  0.606         0.667     1.380\n",
            "41     19     0.895  0.598         0.695     1.346\n",
            "42     20     0.901  0.595         0.679     1.252\n",
            "43     21     0.906  0.573         0.672     1.388\n",
            "44     22     0.910  0.555         0.708     1.321\n",
            "45     23     0.915  0.538         0.690     1.381\n",
            "46     24     0.921  0.515         0.664     1.335\n",
            "47     25     0.919  0.520         0.667     1.507\n",
            "48     26     0.916  0.521         0.649     1.528\n",
            "49     27     0.921  0.497         0.662     1.517\n",
            "50     28     0.932  0.477         0.664     1.486\n",
            "51     29     0.919  0.501         0.690     1.417\n",
            "52     30     0.945  0.445         0.654     1.517\n",
            "Epoch 1/347\n",
            "110/110 [==============================] - 91s 811ms/step - loss: 0.5486 - accuracy: 0.9081 - val_loss: 1.3234 - val_accuracy: 0.6846\n",
            "Epoch 2/347\n",
            "110/110 [==============================] - 87s 794ms/step - loss: 0.5216 - accuracy: 0.9170 - val_loss: 1.3914 - val_accuracy: 0.6641\n",
            "Epoch 3/347\n",
            "110/110 [==============================] - 96s 872ms/step - loss: 0.5516 - accuracy: 0.9036 - val_loss: 1.4014 - val_accuracy: 0.6538\n",
            "Epoch 4/347\n",
            "110/110 [==============================] - 88s 802ms/step - loss: 0.4977 - accuracy: 0.9275 - val_loss: 1.4315 - val_accuracy: 0.6923\n",
            "Epoch 5/347\n",
            "110/110 [==============================] - 88s 799ms/step - loss: 0.4702 - accuracy: 0.9344 - val_loss: 1.3533 - val_accuracy: 0.6897\n",
            "Epoch 6/347\n",
            "110/110 [==============================] - 97s 885ms/step - loss: 0.5315 - accuracy: 0.9104 - val_loss: 1.3827 - val_accuracy: 0.6667\n",
            "Epoch 7/347\n",
            "110/110 [==============================] - 91s 827ms/step - loss: 0.5161 - accuracy: 0.9184 - val_loss: 1.4816 - val_accuracy: 0.6667\n",
            "Epoch 8/347\n",
            "110/110 [==============================] - 87s 794ms/step - loss: 0.4980 - accuracy: 0.9224 - val_loss: 1.4605 - val_accuracy: 0.6667\n",
            "Epoch 9/347\n",
            "110/110 [==============================] - 87s 795ms/step - loss: 0.5083 - accuracy: 0.9227 - val_loss: 1.5817 - val_accuracy: 0.6282\n",
            "Epoch 10/347\n",
            "110/110 [==============================] - 92s 842ms/step - loss: 0.5031 - accuracy: 0.9238 - val_loss: 1.4632 - val_accuracy: 0.6615\n",
            "Epoch 11/347\n",
            "110/110 [==============================] - ETA: 0s - loss: 0.4684 - accuracy: 0.9369"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c6188c71ae9b>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Model weights are saved at the end of every epoch, if it's the best seen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# so far.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stop_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#Try Fit Generator if Data Augmentation is needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1727\u001b[0m                             \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m                         )\n\u001b[0;32m-> 1729\u001b[0;31m                     val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1730\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m                         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "EPOCHS = 400\n",
        "checkpoint_filepath = '/content/drive/MyDrive/Colab Notebooks/Geoguesser/best_model.hdf5'\n",
        "history_file = '/content/drive/MyDrive/Colab Notebooks/Geoguesser/log.csv'\n",
        "\n",
        "#Make History Saver\n",
        "history_callback = keras.callbacks.CSVLogger(history_file, separator=\",\", append=True)\n",
        "\n",
        "#Make Checkpoint\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False)\n",
        "\n",
        "#Make Early Stop\n",
        "early_stop_callback = keras.callbacks.EarlyStopping(\n",
        "\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True)\n",
        "\n",
        "#Try to load the model on the chance that the model has saved progress\n",
        "try:\n",
        "  model = load_model(checkpoint_filepath)\n",
        "  log = pd.read_csv(history_file)\n",
        "\n",
        "  with pd.option_context('display.max_rows', None,\n",
        "                       'display.max_columns', None,\n",
        "                       'display.precision', 3,):\n",
        "    print(log)\n",
        "\n",
        "  EPOCHS = EPOCHS - len(log)\n",
        "\n",
        "#If the saved model doesn't exist, then continue on...\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# Model weights are saved at the end of every epoch, if it's the best seen\n",
        "# so far.\n",
        "history = model.fit(x=X_train, y=Y_train, epochs=EPOCHS, callbacks=[checkpoint_callback,early_stop_callback,history_callback], validation_split=.1)\n",
        "\n",
        "#Try Fit Generator if Data Augmentation is needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UH5zuq-FgnDQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0540aca-05d0-4786-8384-3110266c9407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31/31 [==============================] - 10s 303ms/step\n",
            "Training Accuracy Evaluates to: 0.7183967112024666\n"
          ]
        }
      ],
      "source": [
        "model = load_model('/content/drive/MyDrive/Colab Notebooks/Geoguesser/Best So Far/best_model.hdf5')\n",
        "op = model.predict(X_test)\n",
        "\n",
        "preds = [np.argmax(rw) for rw in op]\n",
        "\n",
        "correct = 0\n",
        "\n",
        "for i,v1 in enumerate(preds):\n",
        "\n",
        "    if(v1 == Y_test[i]):\n",
        "      correct += 1\n",
        "\n",
        "print( \"Training Accuracy Evaluates to: \" + str(correct / len(preds)) )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}